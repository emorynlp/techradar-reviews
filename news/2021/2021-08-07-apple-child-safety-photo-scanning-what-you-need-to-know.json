{
  "source": "https://www.techradar.com/news/apple-child-safety-photo-scanning-what-you-need-to-know",
  "title": "Apple Child Safety photo scanning: what you need to know",
  "category": "phones",
  "updated": "2021-09-03T19:00:10Z",
  "author": "david-lumb",
  "summary": "Will Apple\u2019s Child Safety photo scanning affect you? Read on",
  "article": [
    "Update: After pushback and confusion over how Apple's new policy would work, the company published a multi-page FAQ. It explains precisely where photos will be analyzed for CSAM and how the Messages filtering will work, mostly to reassure users that their entire iCloud storage won't be scanned (just photos right before they're uploaded) and that Messages will only be filtered for users under 12 years old who are linked to family accounts. Apple expressly stated its CSAM photo analysis would not be put to any other use, especially by government request.",
    "Apple announced that it would be enacting a new protocol: automatically scanning iPhones and iPads to check user photos for child sexual assault material (CSAM). The company is doing this to limit the spread of CSAM, but also adding other features \u2018to protect children from predators who use communication tools to recruit and exploit them,\u2019 Apple explained in a blog post. For now, the features will only be available in the US.",
    "Apple will institute a new feature in iOS 15 and iPadOS 15 (both expected to launch in the next couple months) that will automatically scan images on a user\u2019s device to see if they match previously-identified CSAM content, which is identified by unique hashes (e.g. a set of numbers consistent between duplicate images, like a digital fingerprint).",
    "Checking hashes is a common method for detecting CSAM that website security company CloudFlare instituted in 2019 and used by the anti-child sex trafficking nonprofit Thorn, the organization co-founded by Ashton Kutcher and Demi Moore.",
    "In addition, Apple has added two systems parents can optionally enable for children in their family network: first, on-device analysis in the Messages app that scans incoming and outgoing photos for material that might be sexually explicit, which will be blurred by default, and an optional setting can inform account-linked parents if the content is viewed.",
    "Apple is also enabling Siri and Search to surface helpful resources if a user asks about reporting CSAM; both will also intervene when users search queries relating to CSAM, informing the searcher of the material\u2019s harmful potential and pointing toward resources to get help.",
    "That\u2019s an overview of how, by Apple\u2019s own description, it will integrate software to track CSAM and help protect children from predation by intervening when they receive (and send) potentially inappropriate photos. But the prospect of Apple automatically scanning your material has already raised concerns from tech experts and privacy advocates \u2013 we\u2019ll dive into that below.",
    "If you do not have photos with CSAM on your iPhone or iPad, nothing will change for you.",
    "If you do not make a Siri inquiry or online search related to CSAM, nothing will change for you.",
    "If your iPhone or iPad\u2019s account is set up with a family in iCloud and your device is designated as a child in that network, you will see warnings and blurred photos should you receive sexually explicit photos. If your device isn\u2019t linked to a family network as belonging to a child, nothing will change for you.",
    "Lastly, your device won\u2019t get any of these features if you don\u2019t upgrade to iOS 15, iPadOS 15, or macOS Monterey. (The latter will presumably scan iCloud photos for CSAM, but it\u2019s unclear if the Messages intervention for sexually explicit photos will also happen when macOS Monterey users use the app.)",
    "These updates are only coming to users in the US, and it\u2019s unclear when (or if) they\u2019ll be expanded elsewhere \u2013 but given Apple is positioning these as protective measures, we\u2019d be surprised if they didn\u2019t extend it to users in other countries.",
    "From a moral perspective, Apple is simply empowering parents to protect their children and perform a societal service by curbing CSAM. As the company stated in its blog post, \u201cthis program is ambitious, and protecting children is an important responsibility.\u201d",
    "Apple has repeatedly championed the privacy features of its devices, and backs that up with measures like maximizing on-device analysis (rather than uploading data to company servers in the cloud) and secure end-to-end encrypted communications, as well as initiatives like App Tracking Transparency that debuted in iOS 14.5.",
    "But Apple has also been on the receiving end of plenty of lawsuits over the years that have seemingly pushed the company to greater privacy protections \u2013 for instance, a consumer rights advocate in the EU sued the tech giant in November 2020 over Apple\u2019s practice of assigning each iPhone an Identifier for Advertisers (IDFA) to track users across apps, as reported by The Guardian.",
    "This may have nudged Apple to give consumers more control with App Tracking Transparency, or at least aligned with the company\u2019s actions in progress.",
    "TechRadar couldn\u2019t find a particular lawsuit that would have pressured Apple to institute these changes, but it\u2019s entirely possible that the company is proactively protecting itself by giving younger users more self-protection tools as well as eliminating CSAM on its own iCloud servers and iPhones in general \u2013 all of which could conceivably limit Apple\u2019s liability in the future.",
    "But if you can remove CSAM material, why wouldn\u2019t you?",
    "Soon after Apple introduced its new initiatives, security experts and privacy advocates spoke up in alarm \u2013 not, of course, to defend using CSAM but out of concern for Apple\u2019s methods in detecting it on user devices.",
    "The CSAM-scanning feature does not seem to be optional \u2013 it will almost certainly be included in iOS 15 by default, and once downloaded, inextricable from the operating system.",
    "From there, it automatically scans a user\u2019s photos on their device before they\u2019re uploaded to an iCloud account \u2013 if a certain amount of a photo matches those CSAM hashes during a scan, Apple manually reviews the flagged image and, if they determine it to be valid CSAM, the user\u2019s account is shut down and their info is passed along to the National Center for Missing and Exploited Children (NCMEC), which collaborates with law enforcement.",
    "Apple is being very careful to keep user data encrypted and unreadable by company employees unless it breaches a threshold of similarity with known CSAM. And per Apple, \u201cthe threshold is set to provide an extremely high level of accuracy and ensures less than a one in one trillion chance per year of incorrectly flagging a given account.\u201d",
    "But it\u2019s the automatic scanning that has privacy advocates up in arms. \u201cA backdoor is a backdoor,\u201d digital privacy nonprofit Electronic Frontier Foundation (EFF) wrote in its blog post responding to Apple\u2019s initiative, reasoning that even adding this auto-scanning tech was opening the door to potentially broader abuses of access:",
    "\u201cAll it would take to widen the narrow backdoor that Apple is building is an expansion of the machine learning parameters to look for additional types of content, or a tweak of the configuration flags to scan, not just children\u2019s, but anyone\u2019s accounts.",
    "\"That\u2019s not a slippery slope; that\u2019s a fully-built system just waiting for external pressure to make the slightest change,\u201d the EFF wrote, pointing to laws passed in other countries that require platforms to scan user content, like India\u2019s recent 2021 rules.",
    "Others in the tech industry have likewise pushed back against Apple\u2019s auto-scanning initiative, including Will Cathcart, head of the Facebook-owned WhatsApp messaging service.",
    "In a Twitter thread, he pointed to WhatsApp\u2019s practice of making it easier for users to flag CSAM, which he claimed led the service to report over 400,000 cases to NCMEC last year, \u201call without breaking encryption.\u201d",
    "In fairness, Facebook has been trying to get around Apple's App Tracking Transparency: after being forced to disclose how much user data its mobile app (and WhatsApp's app) access, Facebook has tried prompting users to allow that access while criticizing Apple for App Tracking Transparency's harm to small businesses (and, presumably, Facebook) relying on that advertising income.",
    "Other tech experts are waiting for Apple to give more information before they fully side with the EFF\u2019s view.",
    "\u201cThe EFF and other privacy advocates' concern around misuse by authoritarian regimes may be scarily on point or an overreaction - Apple needs to provide more implementation details,\u201d Avi Greengart, founder of tech research and analysis firm Techsponential, told TechRadar via Twitter message.",
    "\u201cHowever, as a parent, I do like the idea that iMessage will flag underage sexting before sending; anything that even temporarily slows the process down and gives kids a chance to think about consequences is a good thing.\u201d"
  ],
  "headers": {
    "7": "Will this affect me?",
    "12": "Why is Apple doing this?",
    "18": "What do security researchers think?"
  },
  "links": {
    "multi-page FAQ": "https://apple.sjv.io/c/221109/473657/7613?subId1=trd-us-2294293197049132300&sharedId=trd-us&u=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2Fpdf%2FExpanded_Protections_for_Children_Frequently_Asked_Questions.pdf",
    "blog post": "https://apple.sjv.io/c/221109/473657/7613?subId1=trd-us-1442992051022357800&sharedId=trd-us&u=https%3A%2F%2Fwww.apple.com%2Fchild-safety%2F",
    "instituted": "https://blog.cloudflare.com/the-csam-scanning-tool/",
    "used": "https://www.thorn.org/blog/hashing-detect-child-sex-abuse-imagery/",
    "App Tracking Transparency": "https://www.techradar.com/how-to/iphone-app-tracking-transparency-how-to-use-the-new-features-in-ios-145",
    "The Guardian": "https://www.theguardian.com/technology/2020/nov/17/apple-faces-privacy-case-in-europe-over-iphone-tracking-id",
    "Electronic Frontier Foundation": "https://www.eff.org/deeplinks/2021/08/apples-plan-think-different-about-encryption-opens-backdoor-your-private-life",
    "India": "https://www.eff.org/deeplinks/2021/07/indias-draconian-rules-internet-platforms-threaten-user-privacy-and-undermine",
    "pointed": "https://twitter.com/wcathcart/status/1423701475595755524",
    "get around": "https://www.techradar.com/news/facebook-warns-ios-14-users-about-new-prompt",
    "much user data": "https://www.techradar.com/news/apples-privacy-labels-reveals-whatsapp-and-facebook-messengers-hunger-for-user-data",
    "prompting": "https://www.techradar.com/news/facebook-really-really-wants-ios-145-users-to-enable-app-tracking",
    "Techsponential": "https://www.techsponential.com/"
  }
}